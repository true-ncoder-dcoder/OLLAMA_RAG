# RAG Creation with Mistral LLM

This repository contains a Jupyter Notebook designed to create Retrieval-Augmented Generation (RAG) using Mistral LLM from OLLAMA. The notebook demonstrates the integration and utilization of the Mistral LLM for enhancing natural language understanding and generation capabilities.

## Prerequisites

Before running the notebook, ensure you have completed the following steps:

1. Install OLLAMA Locally: You need to install OLLAMA on your local machine. Follow the installation steps provided in the OLLAMA repository: OLLAMA Installation Guide.

2. Install Mistral LLM: After installing OLLAMA, open your terminal and execute the following commands to install the Mistral LLM:

```bash
ollama pull mistral
```
3. Serve Mistral LLM: Once Mistral LLM is successfully installed, run it locally by executing:

```bash
ollama serve
```


